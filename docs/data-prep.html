<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Data Prep | OHI Global Assessment Guide</title>
  <meta name="description" content="A guide for conducting the OHI Global Assessment" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Data Prep | OHI Global Assessment Guide" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A guide for conducting the OHI Global Assessment" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Data Prep | OHI Global Assessment Guide" />
  
  <meta name="twitter:description" content="A guide for conducting the OHI Global Assessment" />
  

<meta name="author" content="OHI Team" />


<meta name="date" content="2019-08-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="starting-a-new-year.html">
<link rel="next" href="updating-scores.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="lib/css/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" href="lib/css/style.css" type="text/css" />
<link rel="stylesheet" href="lib/css/lesson.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">OHI Global Guide</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Overview</a></li>
<li class="chapter" data-level="2" data-path="starting-a-new-year.html"><a href="starting-a-new-year.html"><i class="fa fa-check"></i><b>2</b> Starting a new year</a><ul>
<li class="chapter" data-level="2.1" data-path="starting-a-new-year.html"><a href="starting-a-new-year.html#review-and-organize-relevant-issues"><i class="fa fa-check"></i><b>2.1</b> Review and organize relevant issues</a></li>
<li class="chapter" data-level="2.2" data-path="starting-a-new-year.html"><a href="starting-a-new-year.html#prepare-the-repositories"><i class="fa fa-check"></i><b>2.2</b> Prepare the repositories!</a><ul>
<li class="chapter" data-level="2.2.1" data-path="starting-a-new-year.html"><a href="starting-a-new-year.html#ohiprep-repository"><i class="fa fa-check"></i><b>2.2.1</b> ohiprep repository</a></li>
<li class="chapter" data-level="2.2.2" data-path="starting-a-new-year.html"><a href="starting-a-new-year.html#ohi-global"><i class="fa fa-check"></i><b>2.2.2</b> ohi-global</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="starting-a-new-year.html"><a href="starting-a-new-year.html#keeping-track-of-layer-progress"><i class="fa fa-check"></i><b>2.3</b> Keeping track of layer progress</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-prep.html"><a href="data-prep.html"><i class="fa fa-check"></i><b>3</b> Data Prep</a><ul>
<li class="chapter" data-level="3.1" data-path="data-prep.html"><a href="data-prep.html#file-organization"><i class="fa fa-check"></i><b>3.1</b> File organization</a><ul>
<li class="chapter" data-level="3.1.1" data-path="data-prep.html"><a href="data-prep.html#saving-external-data"><i class="fa fa-check"></i><b>3.1.1</b> Saving external data</a></li>
<li class="chapter" data-level="3.1.2" data-path="data-prep.html"><a href="data-prep.html#vpn-setup-to-access-mazu"><i class="fa fa-check"></i><b>3.1.2</b> VPN Setup to Access Mazu</a></li>
<li class="chapter" data-level="3.1.3" data-path="data-prep.html"><a href="data-prep.html#setting-up-rstudio-mazu-server"><i class="fa fa-check"></i><b>3.1.3</b> Setting up Rstudio Mazu Server</a></li>
<li class="chapter" data-level="3.1.4" data-path="data-prep.html"><a href="data-prep.html#using-cyberduck-to-transfer-large-files"><i class="fa fa-check"></i><b>3.1.4</b> Using Cyberduck to Transfer Large Files</a></li>
<li class="chapter" data-level="3.1.5" data-path="data-prep.html"><a href="data-prep.html#globalprep-files"><i class="fa fa-check"></i><b>3.1.5</b> globalprep files</a></li>
<li class="chapter" data-level="3.1.6" data-path="data-prep.html"><a href="data-prep.html#intermediate-files-that-are-too-large-for-github"><i class="fa fa-check"></i><b>3.1.6</b> Intermediate files that are too large for Github</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-prep.html"><a href="data-prep.html#starting-a-new-data-prep-project"><i class="fa fa-check"></i><b>3.2</b> Starting a new data prep project</a><ul>
<li class="chapter" data-level="3.2.1" data-path="data-prep.html"><a href="data-prep.html#prepare-the-dataprep-folder"><i class="fa fa-check"></i><b>3.2.1</b> Prepare the dataprep folder</a></li>
<li class="chapter" data-level="3.2.2" data-path="data-prep.html"><a href="data-prep.html#start-an-issue"><i class="fa fa-check"></i><b>3.2.2</b> Start an issue</a></li>
<li class="chapter" data-level="3.2.3" data-path="data-prep.html"><a href="data-prep.html#update-the-files"><i class="fa fa-check"></i><b>3.2.3</b> Update the files</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="data-prep.html"><a href="data-prep.html#a-typical-data-prep-script"><i class="fa fa-check"></i><b>3.3</b> A typical data prep script</a></li>
<li class="chapter" data-level="3.4" data-path="data-prep.html"><a href="data-prep.html#prep-rmd-1.-summary"><i class="fa fa-check"></i><b>3.4</b> prep Rmd: 1. Summary</a></li>
<li class="chapter" data-level="3.5" data-path="data-prep.html"><a href="data-prep.html#prep-rmd-2.-updates"><i class="fa fa-check"></i><b>3.5</b> prep Rmd: 2. Updates</a></li>
<li class="chapter" data-level="3.6" data-path="data-prep.html"><a href="data-prep.html#prep-rmd-3.-data-sources"><i class="fa fa-check"></i><b>3.6</b> prep Rmd: 3. Data sources</a></li>
<li class="chapter" data-level="3.7" data-path="data-prep.html"><a href="data-prep.html#prep-rmd-4.-set-up"><i class="fa fa-check"></i><b>3.7</b> prep Rmd: 4. Set up</a><ul>
<li class="chapter" data-level="3.7.1" data-path="data-prep.html"><a href="data-prep.html#packages"><i class="fa fa-check"></i><b>3.7.1</b> Packages</a></li>
<li class="chapter" data-level="3.7.2" data-path="data-prep.html"><a href="data-prep.html#common.r"><i class="fa fa-check"></i><b>3.7.2</b> common.R</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="data-prep.html"><a href="data-prep.html#prep-rmd-5.-data-prep"><i class="fa fa-check"></i><b>3.8</b> prep Rmd: 5. Data prep</a></li>
<li class="chapter" data-level="3.9" data-path="data-prep.html"><a href="data-prep.html#prep-rmd-6.-gapfilling"><i class="fa fa-check"></i><b>3.9</b> prep Rmd: 6. Gapfilling</a><ul>
<li class="chapter" data-level="3.9.1" data-path="data-prep.html"><a href="data-prep.html#training-linear-models-for-gapfilling"><i class="fa fa-check"></i><b>3.9.1</b> Training linear models for gapfilling</a></li>
<li class="chapter" data-level="3.9.2" data-path="data-prep.html"><a href="data-prep.html#saving-gapfilling-report-files"><i class="fa fa-check"></i><b>3.9.2</b> Saving gapfilling report files</a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="data-prep.html"><a href="data-prep.html#prep-rmd-7.-results-check"><i class="fa fa-check"></i><b>3.10</b> prep Rmd: 7. Results check</a></li>
<li class="chapter" data-level="3.11" data-path="data-prep.html"><a href="data-prep.html#prep-rmd-8.-final-run"><i class="fa fa-check"></i><b>3.11</b> prep Rmd: 8. Final run</a></li>
<li class="chapter" data-level="3.12" data-path="data-prep.html"><a href="data-prep.html#notes-on-parallel-processing"><i class="fa fa-check"></i><b>3.12</b> Notes on parallel processing</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="updating-scores.html"><a href="updating-scores.html"><i class="fa fa-check"></i><b>4</b> Updating scores</a></li>
<li class="chapter" data-level="5" data-path="finalizing-the-global-assessment.html"><a href="finalizing-the-global-assessment.html"><i class="fa fa-check"></i><b>5</b> Finalizing the global assessment</a><ul>
<li class="chapter" data-level="5.1" data-path="finalizing-the-global-assessment.html"><a href="finalizing-the-global-assessment.html#create-results-folder"><i class="fa fa-check"></i><b>5.1</b> Create results folder</a></li>
<li class="chapter" data-level="5.2" data-path="finalizing-the-global-assessment.html"><a href="finalizing-the-global-assessment.html#final-score-review"><i class="fa fa-check"></i><b>5.2</b> Final score review</a></li>
<li class="chapter" data-level="5.3" data-path="finalizing-the-global-assessment.html"><a href="finalizing-the-global-assessment.html#methods-document"><i class="fa fa-check"></i><b>5.3</b> Methods document</a><ul>
<li class="chapter" data-level="5.3.1" data-path="finalizing-the-global-assessment.html"><a href="finalizing-the-global-assessment.html#steps-to-updating-methods-document"><i class="fa fa-check"></i><b>5.3.1</b> Steps to updating methods document</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="finalizing-the-global-assessment.html"><a href="finalizing-the-global-assessment.html#results-document"><i class="fa fa-check"></i><b>5.4</b> Results document</a></li>
<li class="chapter" data-level="5.5" data-path="finalizing-the-global-assessment.html"><a href="finalizing-the-global-assessment.html#updates-document"><i class="fa fa-check"></i><b>5.5</b> Updates document</a></li>
<li class="chapter" data-level="5.6" data-path="finalizing-the-global-assessment.html"><a href="finalizing-the-global-assessment.html#upload-to-a-repository"><i class="fa fa-check"></i><b>5.6</b> Upload to a repository</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">OHI Global Assessment Guide</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data-prep" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Data Prep</h1>
<p>All data layers are prepared in the ohiprep_v20?? repository.</p>
<p>Spend some time researching the data prior to launching into the coding in R. I look over the files in the dataprep folder on Github, as well as the Mazu folder that contains the raw data. I research the specific data layers I will be working on and the goals/pressures/resilience dimensions they are used to calculate. I typically start in the <a href="https://raw.githack.com/OHI-Science/ohi-global/published/global_supplement/Supplement.html">methods</a> document. I also research the source data, looking over websites, reports, and published papers.</p>
<p>In regard to preparing the data, the best approach is to prepare the layer or layers within a single Rmd script and then update the OHI scores one layer at a time. This approach makes it much easier to identify and track potential errors.</p>
<p>This section will discuss:</p>
<ol style="list-style-type: decimal">
<li>File organization</li>
<li>Starting a new data prep project</li>
<li>Anatomy of a typical data prep script</li>
<li>Notes on parallel processing</li>
</ol>
<hr />
<div id="file-organization" class="section level2">
<h2><span class="header-section-number">3.1</span> File organization</h2>
<!--- TODO Ask Mel what "be responsible for serving other people's data" means. Unclear to me. --->
<div id="saving-external-data" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Saving external data</h3>
<p>In almost all cases, OHI data comes from other institutions. We save these data to<br />
the NCEAS private server (Mazu) because we do not want to be responsible for serving other people’s data.</p>
<p>These data are saved to Mazu: git-annex/globalprep/_raw_data in a folder that is labeled with an abbreviated version of the datasource (Figure 1). The data is saved to a folder describing the year the data was downloaded (e.g., d2015, d2016).</p>
<p>Figure 1: Location of raw data saved to Mazu.</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vT09zXxdCewmMAuDscXvS-pAoYM92AMd8Xxw_DhP93IFirHRtioAu19IsjEUN24091Cvm7vmCkB8piM/pub?w=712&amp;h=531" /></p>
<p>Every raw data folder should have a README.md (keep the caps so it is consistent and easy to see). *Note we are using .md rather than .txt even for READMEs on Mazu.</p>
<p>Each README should include the following (<a href="https://github.com/OHI-Science/ohiprep/blob/master/src/templates/generic_raw_data_README.md">template</a>):</p>
<ul>
<li>Detailed source information. For example:
<ul>
<li>full paper citation and link for publication</li>
<li>Link to online data source</li>
<li>Full email history with data provider</li>
</ul></li>
<li>If it was downloaded online, provided written and visual instructions so that the reader can mimic your same steps to get the same data. Include screenshots if possible!</li>
<li>Version information for data</li>
<li>Years included in the datatset</li>
<li>Year the data was published</li>
<li>Type of data included in the dataset (e.g., catch per species (tons) per country)</li>
<li>Any other information that could possibly be useful to anyone</li>
</ul>
<hr />
</div>
<div id="vpn-setup-to-access-mazu" class="section level3">
<h3><span class="header-section-number">3.1.2</span> VPN Setup to Access Mazu</h3>
<p>Here is why you will want to get your VPN accounts set up:</p>
<ol style="list-style-type: decimal">
<li>You can access files on Mazu while you are offsite</li>
<li>You can run intensive memory projects on the Mazu server while you are offsite</li>
<li>You will functionally have two computers, so you can set up a long-running processes (on the Mazu server) and then you can work from your personal computer</li>
</ol>
<p>This is especially important when we start working with spatial files. It is impossible to run the spatial analyses on a personal computer due to speed and memory limitations.</p>
<p>The VPN allows you to work from the Mazu server when you are not onsite. I almost exclusively work from Mazu. Basically, when I log in, my computer thinks I am working from NCEAS. I can access the Mazu drive. My IP address is through NCEAS and my clock displays the time in Santa Barbara.</p>
<p>Here is some information on getting setup: <a href="http://www.ets.ucsb.edu/services/campus-vpn/what-vpn" class="uri">http://www.ets.ucsb.edu/services/campus-vpn/what-vpn</a>
and: <a href="http://www.ets.ucsb.edu/services/campus-vpn/get-connected" class="uri">http://www.ets.ucsb.edu/services/campus-vpn/get-connected</a></p>
<p>Once you have the VPN, here’s how you map Mazu from a Mac:</p>
<p>Finder, then do command-K (Or go to the Menu &gt; Go &gt; Connect to Server)
Paste:</p>
<pre><code>smb://mazu.nceas.ucsb.edu/ohi</code></pre>
<p>and then hit Connect!</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vRctPyg0LHmcWcvMRJzeBgDIuMZYjCJS2kBYzq8sWIDw98u6nW4cbKBe4h_gaH4QbgCdNdv3plxWTID/pub?w=960&amp;h=720" />
***</p>
</div>
<div id="setting-up-rstudio-mazu-server" class="section level3">
<h3><span class="header-section-number">3.1.3</span> Setting up Rstudio Mazu Server</h3>
<p>You will definitely want to get set up to run RStudio on Mazu! This is so great for so many reasons.</p>
<ol style="list-style-type: decimal">
<li>It makes it super easy to work remotely.</li>
<li>If you are running long R processes, Mazu is faster and it will not hog your computer’s resources.</li>
<li>It is possible to run scripts on Mazu’s multiple cores (which is necessary to run many of our scripts).</li>
</ol>
<p>You need to contact <a href="mailto:support@nceas.ucsb.edu" class="email">support@nceas.ucsb.edu</a> to get this set up.</p>
<p>This is what it looks like on my computer:
<img src="https://docs.google.com/drawings/d/e/2PACX-1vQMt-gUz07-sdOmHiWQn2fKGb7lOPChXwsoExZ6NiCVu0ylHv65rV3e5H6pWwic4FDas1ajjZSj4v8I/pub?w=960&amp;h=720" /></p>
<hr />
</div>
<div id="using-cyberduck-to-transfer-large-files" class="section level3">
<h3><span class="header-section-number">3.1.4</span> Using Cyberduck to Transfer Large Files</h3>
<p>You should consider looking into Cyberduck (but I wouldn’t bother with this until you find you need it).</p>
<p>If you need to move really big files between Mazu and your computer, I find that the VPN typically fails. For this, I use <a href="https://download.cnet.com/Cyberduck-for-Windows/3000-18500_4-75623481.html">Cyberduck</a>.</p>
<ol style="list-style-type: decimal">
<li><p>After you download Cyberduck, open it up and click Open Connection.
<img src="https://docs.google.com/drawings/d/e/2PACX-1vRkVsbyCsv3LQYuSbAueUjv2KLyiJCWL9gvJd3jCIyX-xvCHnWzWJU0gO-izhaurjdM8S5yHOCQCR_r/pub?w=960&amp;h=720" /></p></li>
<li><p>Type in the server address mazu.nceas.ucsb.edu, your sign-in credentials, select SFTP (SSH File Transfer Protocol) in the drop down at the top (supposed to be safer), AND type in /home/shares/ohi/git-annex under “Path:”. Click Connect.
<img src="https://docs.google.com/drawings/d/e/2PACX-1vSXABa8ABX-dHcOA3_--0k_y1rZCWrFuVgx2GIurOIqSRKoyZGZ7Ec0Ixt0NGlx7y6NcuB10rzfXm3A/pub?w=960&amp;h=720" /></p></li>
</ol>
<p><em>Note:</em> You can modify the path if you want to be directed to another location in mazu.</p>
<ol start="3" style="list-style-type: decimal">
<li>Now you can start dragging and dropping! Click Disconnect when you’re done using Cyberduck.
***</li>
</ol>
</div>
<div id="globalprep-files" class="section level3">
<h3><span class="header-section-number">3.1.5</span> globalprep files</h3>
<p>All of the R scripts and metadata used to prepare the data, as well as the final data layers are saved in the Github ohiprep_v???? repository in the globalprep folder.</p>
<p>The only data that will not be saved on Github are files that are too large or incompatible with Github (see below).</p>
<p><strong>Primary goal/component folder</strong> The folder should be named according to the OHI target (the goal or dimension that the data is used to calculate). For example the folder for the tourism and recreation goal would be called: globalprep/tr (see table below for all folder abbreviations). These recommendations are flexible and should be modified as needed, for example goals can be combined in a single folder (e.g., spp_ico) or, there may be several folders for different components of a single goal (e.g. tr_sustainability and tr_tourists).</p>
<table>
<thead>
<tr class="header">
<th>target</th>
<th>suggested folder name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Artisanal Fishing Opportunity</td>
<td>ao</td>
</tr>
<tr class="even">
<td>Carbon Storage</td>
<td>cs</td>
</tr>
<tr class="odd">
<td>Clean Waters</td>
<td>cw</td>
</tr>
<tr class="even">
<td>Coastal Protection</td>
<td>cp</td>
</tr>
<tr class="odd">
<td>Coastal Livelihoods</td>
<td>liv</td>
</tr>
<tr class="even">
<td>Coastal Economies</td>
<td>eco</td>
</tr>
<tr class="odd">
<td>Fisheries</td>
<td>fis</td>
</tr>
<tr class="even">
<td>Habitats</td>
<td>hab</td>
</tr>
<tr class="odd">
<td>Iconic Species</td>
<td>ico</td>
</tr>
<tr class="even">
<td>Lasting Special Places</td>
<td>lsp</td>
</tr>
<tr class="odd">
<td>Mariculture</td>
<td>mar</td>
</tr>
<tr class="even">
<td>Natural Products</td>
<td>np</td>
</tr>
<tr class="odd">
<td>Species</td>
<td>spp</td>
</tr>
<tr class="even">
<td>Tourism and Recreation</td>
<td>tr</td>
</tr>
<tr class="odd">
<td>Pressure</td>
<td>prs_<em>additional_pressure_id</em></td>
</tr>
<tr class="even">
<td>Resilience</td>
<td>res_<em>additional_resilience_id</em></td>
</tr>
</tbody>
</table>
<p>This folder will contain:</p>
<ul>
<li><p>a <em>README.md</em> that will link to the appropriate information pages on ohi-science.org The README.md should follow this <a href="https://github.com/OHI-Science/ohiprep/blob/master/src/templates/generic_readme.md">template</a>.</p></li>
<li><p><strong>Year-specific folders</strong> within the goal/component folder organize output by assessment year (v2015, v2016). Each of the assessment year folders should have:
* a README.md (see <a href="https://github.com/OHI-Science/ohiprep/blob/master/src/templates/generic_readme_year.md">this template</a>)
* a data_prep.R, or .Rmd that is well-documented. <a href="https://github.com/OHI-Science/ohiprep/blob/master/src/templates/generic_data_prep.Rmd">Here is the dataprep template</a>.
* a series of folders to organize data that include:
+ <code>raw</code> for ‘raw-ish’ type files that would not be on the server. This is typically for piecemeal raw data that we compile (e.g., U.S. State Department travel warnings), and not data we have downloaded from a single source (which would go on Mazu). In most cases, this folder will not be used.
+ <code>int</code> for intermediate files (previously we’ve used tmp, working, or other naming conventions so this might not be entirely consistent).
+ <code>output</code> for the final data layer that is used in the OHI toolbox.</p></li>
</ul>
<p>The final datasets (the ones stored in the <code>output</code> folder) will be preceeded by the target abbreviation followed by an underscore that provides a brief description of the data, e.g., tr_sustainability.csv).</p>
<!--- TODO Slightly confused as to what this image is supposed to be showing us. Is this showing corrections that should be made to mistakes? --->
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vQL9FTF2lZ3yBCv9ZEI7OQ0sNCGdKpUrP0FfjtV_14vvzHAx3rxMQLJ-nFcfjRVT61sHTti4Husmrm-/pub?w=960&amp;h=720" /></p>
<hr />
</div>
<div id="intermediate-files-that-are-too-large-for-github" class="section level3">
<h3><span class="header-section-number">3.1.6</span> Intermediate files that are too large for Github</h3>
<p>These files will be saved on Mazu, the internal server’s, globalprep folder.</p>
<p>Our goal is to have everything (except for data obtained from other sources) stored on GitHub, but some files are too large or inappropriate for GitHub and must be stored on Mazu. Each of these files should be stored in a way that mirrors that on Github. If there is a need to make a duplicate folder on <code>git-annex</code>, it should have the same name as the folder used in GitHub.</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vQjq4gTVyHmB-QKIAAWEin3YRPSsEqoq2yZZ0W5uP6D47vXAZUoNJl21BPKLumifz_UgbZ_ef1-U0lt/pub?w=960&amp;h=720" /></p>
<p>Store any intermediate or final output files that are too large for github in these folders. Keep the same subfolder structure. If you are working in <code>spp_ico</code> and have temporary rasters to store on Mazu, save them in a folder named <code>int</code>.</p>
<p><strong>Raw data should not be stored here. This should be stored in Mazu’s <code>_raw_data</code> folder</strong></p>
<hr />
</div>
</div>
<div id="starting-a-new-data-prep-project" class="section level2">
<h2><span class="header-section-number">3.2</span> Starting a new data prep project</h2>
<div id="prepare-the-dataprep-folder" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Prepare the dataprep folder</h3>
<p>In ohiprep_v20??/globalprep navigate to the folder that contains the files you will need to prepare the data. Select the most recent assessment year of data and copy/paste to create a new folder in the same location, changing the name to reflect the current assessment year.</p>
<p>For example, if we are preparing data layers for the 2019 assessment for the artisanal opportunities goal, ao, we would copy the v2018 folder and name it v2019.</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vTkXdx-bGQoOhzXn6Wz94yV9_S9BlKY0kPhAtp9nKaewr0lRD0Cr0B_H37aUoDANt0Ck6p4wSaFIJPz/pub?w=960&amp;h=720" />
Next, delete all unnecessary files, particularly the files in the raw, intermediate, and output folders! New versions of these data will be created in the updated data_prep script, and you want to be sure that this is indeed what happens. If there are data already in the folder it can be easy to overlook a mistake, such as not using the correct file path to save the data, etc..</p>
<p>Typically, you will not delete the dataprep script/s and README. You will also want to preserve the folder structure. If in doubt, I delete the files and then copy them over as needed. For some goals, such as fisheries, there are lots and lots of files and it is very confusing. In these cases, I just copy the files as I need them.</p>
<p>Once you have created and cleaned the new folder commit and push the updates!</p>
</div>
<div id="start-an-issue" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Start an issue</h3>
<p>On Github create an issue for this dataprep project. Here you will document progress, report problems or concerns, pose questions, describe changes to code, and report on final results. For example, if working on the artisanal opportunities dataprep, name the issue “Artisanal opportunities”.</p>
<p>Once the relevant data layers are created and incorporated into the OHI scores this issue will be closed.</p>
</div>
<div id="update-the-files" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Update the files</h3>
<p>Carefully walk through the data prep script/s and update as necessary.</p>
<p>You will also need to update READMEs.</p>
<p>As you check and update the scripts, don’t assume everything is correct. There could be changes to the source data that introduce errors (this often happens). We could have made a mistake somewhere. And, even if there aren’t any mistakes, there is usually room for improvement.</p>
<p>Checking the dataprep scripts involves:</p>
<ul>
<li>Going through each line of code to understand what is happening. Each step of dplyr chains should be evaluated, rather than running the entire chain and assuming it is correct.</li>
<li>Using functions such as summary, dim, length functions and figures to explore the data at different stages of preparation.</li>
<li>Check the dimensions of the dataframe after each join or spread/collapse to ensure the length of the updated data makes sense.</li>
<li>Check that the frequency of NA values seems reasonable, especially after joins and spread/collapse functions. And, if you gapfill missing values, make sure that you end up estimating all NA values.</li>
</ul>
<hr />
</div>
</div>
<div id="a-typical-data-prep-script" class="section level2">
<h2><span class="header-section-number">3.3</span> A typical data prep script</h2>
<p>All data prep is performed in R or, preferrably, Rmd documents. Rmd is an ideal format because it seemlessly integrates code and documentation, can display figures, and the output provides a clean methods document.</p>
<p>We have several shared practices for preparing data:</p>
<ul>
<li>Ideally Rmd/R files are used to download and save source datafiles, but this isn’t possible in most cases due to the format of the data.</li>
<li>We put a large premium on documenting the steps used to prepare data!</li>
<li>In many cases, the data preparation for a goal is performed in a single file. But, for more complex goals it is better to break the data preparation into multiple Rmd documents. If multiple Rmd documents are used, a README must describe what each Rmd document does, the input/outputs, and the order of processing.<br />
</li>
<li>If a process is run multiple times, the code should be converted to a function and placed in folder labeled R or src.</li>
<li>The <code>here</code> package, and <code>here()</code> function should be used control file paths.</li>
<li>We use the tidyverse for tidying data</li>
</ul>
<p>A typical data prep script (or series of scripts) will include the following sections, which are described in more detail below:</p>
<ol style="list-style-type: decimal">
<li>Summary: general description of the data prepared by the script</li>
<li>Updates: updates to source data and/or methods from previous year</li>
<li>Data sources: description of source data</li>
<li>Set up: code chunk that loads relevant R packages and functions</li>
<li>Data prep: code chunks for preparing the data, this is typically the bulk of the Rmd file</li>
<li>Gapfilling: code chunks for estimating and documenting missing data</li>
<li>Results check: code used to check results</li>
<li>Final run: a final run of all the code after restarting R</li>
</ol>
<p>A generic data prep Rmd file is located on Github: <a href="https://raw.githubusercontent.com/OHI-Science/ohiprep/master/src/templates/generic_data_prep.Rmd">https://raw.githubusercontent.com/OHI-Science/ohiprep/master/src/templates/generic_data_prep.Rmd</a></p>
</div>
<div id="prep-rmd-1.-summary" class="section level2">
<h2><span class="header-section-number">3.4</span> prep Rmd: 1. Summary</h2>
<p>This section describes the purpose of the script and the data layers that are generated.</p>
<p>For example, the 2019 AO summary looks like this:</p>
<ul>
<li>“This script generates the”need" layer for the artisanal opportunities goal. The “access” layer, which is not updated due to a lack of a data source, is located here: globalprep/res_mora_ao/v2013/data/r_mora_s4_2013a.csv."</li>
</ul>
</div>
<div id="prep-rmd-2.-updates" class="section level2">
<h2><span class="header-section-number">3.5</span> prep Rmd: 2. Updates</h2>
<p>This section describes all the updates to source data and/or methods since the previous year’s assessment.</p>
<p>For example, the 2019 AO updates look like this:</p>
<ul>
<li>“Uninhabited and low population regions now given an NA value (vs. a gapfilled value).”</li>
<li>“Added an additional year of GDP data from the World Bank”</li>
</ul>
</div>
<div id="prep-rmd-3.-data-sources" class="section level2">
<h2><span class="header-section-number">3.6</span> prep Rmd: 3. Data sources</h2>
<p>This section describes all data sources and may include:</p>
<p><strong>Reference</strong>: [citation for source data; website, literature, contact information. Version of data (if relevant).]</p>
<p><strong>Downloaded</strong>: [date downloaded or received]</p>
<p><strong>Description</strong>: [e.g., surface aragonite state]</p>
<p><strong>Native data resolution</strong>: [e.g., 1 degree, 30 m, country, etc.]</p>
<p><strong>Time range</strong>: [e.g., 1880-1899, monthly data provided for each year]</p>
<p><strong>Format</strong>: [e.g., NetCDF, Excel file]
<br> <br></p>
<p>For example, the 2019 AO data sources section looks like this:</p>
<p><strong>Downloaded</strong>: 7/23/2018</p>
<p><strong>Description</strong>:<br />
GDP adjusted per capita by PPP (ppppcgdp)
<a href="http://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD" class="uri">http://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD</a>
Reported at country scale.</p>
<p>GDP per capita based on purchasing power parity (PPP). PPP GDP is gross domestic product converted to international dollars using purchasing power parity rates. An international dollar has the same purchasing power over GDP as the U.S. dollar has in the United States. GDP at purchaser’s prices is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in constant international dollars based on the 2011 ICP round.</p>
<p>Data is available directly to R through the WDI package.</p>
<p><strong>Time range</strong>: 1990-2017</p>
</div>
<div id="prep-rmd-4.-set-up" class="section level2">
<h2><span class="header-section-number">3.7</span> prep Rmd: 4. Set up</h2>
<p>This code chunk is used to load packages, source functions, and set variables used throughout the analyses.</p>
<div id="packages" class="section level3">
<h3><span class="header-section-number">3.7.1</span> Packages</h3>
<p>The packages we load depend on the analyses, but we always use:</p>
<ul>
<li>dplyr and tidyr: data wrangling tools <a href="https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf">a cheatsheet</a></li>
<li><a href="https://github.com/jennybc/here_here">here</a>: controls file paths</li>
</ul>
<p>We often use the following for spatial analyses:</p>
<ul>
<li>rgdal: tools for dealing with coordinate reference systems</li>
<li>sp: classes and methods for spatial data</li>
<li>sf: a new version of sp, providing a standardized way to encode spatial vector data</li>
<li>raster: reading, writing, manipulating, analyzing and modeling of gridded spatial data</li>
<li>fasterize: a better way to convert a shapefile to a raster file</li>
</ul>
<p>We often use the zoo package for time-series data.</p>
<p>And, for parallel processing, typically used to perform spatial analyses, we use:</p>
<ul>
<li>doParallel</li>
<li>foreach</li>
</ul>
</div>
<div id="common.r" class="section level3">
<h3><span class="header-section-number">3.7.2</span> common.R</h3>
<p>Nearly all scripts will source a common.R file. This file creates several objects that make it easier to conduct an OHI assessment. This includes:</p>
<table>
<colgroup>
<col width="21%" />
<col width="78%" />
</colgroup>
<thead>
<tr class="header">
<th>object</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>dir_M</td>
<td>file path to Mazu</td>
</tr>
<tr class="even">
<td>mollCRS</td>
<td>crs code for the mollweide coordinate refernce system we use in the global assessment</td>
</tr>
<tr class="odd">
<td>regions_shape()</td>
<td>A function to load a simple feature object called “regions” with polygons for land/eez/highseas/antarctica regions. The “regions” object uses the Mollweide coordinate reference system.</td>
</tr>
<tr class="even">
<td>ohi_rasters()</td>
<td>function to load two rasters: global eez regions and ocean region</td>
</tr>
<tr class="odd">
<td>rgn_data()</td>
<td>function to load 2 dataframes describing global regions</td>
</tr>
<tr class="even">
<td>rgn_syns()</td>
<td>function to load dataframe of region synonyms (used to convert country names to OHI regions)</td>
</tr>
<tr class="odd">
<td>low_pop()</td>
<td>function to load dataframe of regions with low and no human population</td>
</tr>
<tr class="even">
<td>UNgeorgn()</td>
<td>function to load dataframe of UN sociopolitical regions, typically used to gapfill missing data</td>
</tr>
</tbody>
</table>
<p>To load the data in a data function:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&#39;http://ohi-science.org/ohiprep_v2019/workflow/R/common.R&#39;</span>)</code></pre>
<pre><code>## This file makes it easier to process data for the OHI global assessment
##  by creating the following objects:
## 
##  * dir_M = identifies correct file path to Mazu (internal server) based on your operating system
##  * mollCRS = the crs code for the mollweide coordinate reference system we use in the global assessment
##  * regions_shape() = function to load global shapefile for land/eez/high seas/antarctica regions
##  * ohi_rasters() = function to load two rasters: global eez regions and ocean region
##  * region_data() = function to load 2 dataframes describing global regions 
##  * rgn_syns() = function to load dataframe of region synonyms (used to convert country names to OHI regions)
##  * low_pop() = function to load dataframe of regions with low and no human population
##  * UNgeorgn = function to load dataframe of UN geopolitical designations used to gapfill missing data</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># call the function to load the data, the message describes the available data: </span>
<span class="kw">region_data</span>()</code></pre>
<pre><code>## loads 2 dataframes: rgns_all and rgns_eez 
##  rgns_all = includes eez/high seas/antarctica regions, IDs correspond with region shapefile and raster
##  rgns_eez = includes only eez regions</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(rgns_all)</code></pre>
<pre><code>##   rgn_type type_w_ant rgn_id rgn_ant_id              rgn_name
## 1      eez        eez      1          1         Cocos Islands
## 2      eez        eez     10         10                 Nauru
## 3      eez        eez    100        100   Republique du Congo
## 4      eez        eez    101        101               Namibia
## 5      eez        eez    102        102          South Africa
## 6      eez        eez    103        103 Sao Tome and Principe</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(rgns_eez)</code></pre>
<pre><code>##   rgn_id         rgn_name eez_iso3 territory admin_rgn_id
## 1      1    Cocos Islands      CCK       yes           16
## 2      2 Christmas Island      CXR       yes           16
## 3      3   Norfolk Island      NFK       yes           16
## 4      4 Macquarie Island      AUS       yes           16
## 5      5    New Caledonia      NCL       yes          179
## 6      6          Vanuatu      VUT        no            6
##   admin_country_name Notes
## 1          Australia      
## 2          Australia      
## 3          Australia      
## 4          Australia      
## 5             France      
## 6            Vanuatu</code></pre>
<div id="metadata-for-common.r" class="section level4">
<h4><span class="header-section-number">3.7.2.1</span> metadata for common.R</h4>
<p><em>dir_M and mollCRS</em></p>
<p>The following are the dir_M and mollCRS objects:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">## dir_M describes the path to our internal server based on your computer&#39;s operating system</span>
<span class="co">## </span><span class="al">NOTE</span><span class="co">: The following may be different on your operating system</span>
dir_M</code></pre>
<pre><code>## [1] &quot;/home/shares/ohi&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">## mollCRS is the code for the Mollweide coordinate reference system</span>
mollCRS</code></pre>
<pre><code>## CRS arguments:
##  +proj=moll +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +units=m +no_defs</code></pre>
<p><em>regions_shape</em></p>
<p>The regions_shape function returns a simple feature object called “regions”. Regions is the master global shapefile that includes polygons for land, eez, high seas, and antarctica regions in the Mollweide coordinate reference system.</p>
<p>Sometimes it is necessary to convert from a simple feature object to a shapefile object because some functions still do not work with simple feature objects, or, if the spatial file is modified, a saved shapefile may be desired. This is accomplished like this:</p>
<p><code>regions_shape &lt;- as(regions, "Spatial")</code></p>
<!--- TODO Can't get any of these code chunks to run. Are they necessary to run? Need to discuss with Mel--->
<p>The regions file with eez (dark blue), fao or high seas (light blue), and antarctica or CCAMLR (green) regions.</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vRpNcorAKw0Hzk--d1R_llBw4-qeCx1mcgDSpaUeIASO2YDBIhPcoEXMshIFHgo9nv8E6_o6h70fs-P/pub?w=960&amp;h=720" /></p>
<p>The regions object is a simple feature multipolygon spatial object in the Mollweide coordinate reference system. There are 7 fields described in the table</p>
<table>
<colgroup>
<col width="13%" />
<col width="23%" />
<col width="22%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>field</th>
<th>data type</th>
<th>description</th>
<th>examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>type_w_ant</td>
<td>factor</td>
<td>identifies all polygons as eez, fao (high seas), ccamlr (antarctica), or land</td>
<td>eez (n=220), fao (15), eez-ccamlr (19), land (220), land-ccamlr (9), eez-disputed (1), land-disputed (1), eez-inland (3), land-noeez (38)</td>
</tr>
<tr class="even">
<td>rgn_type</td>
<td>factor</td>
<td>similar to type_w_ant, but does not specify eez/ccamlr and land/land-ccamlr regions</td>
<td>eez (n=239), fao (15), land (229), eez-disputed (1), land-disputed (1), eez-inland (3), land-noeez (38)</td>
</tr>
<tr class="odd">
<td>rgn_ant_id</td>
<td>numeric</td>
<td>region ids</td>
<td>1-250 country land and eez (these are the official global regions; some numbers are skipped); 255 disputed land and eez; 260-277 fao high seas; 301-337 (country land, no eez); 248100-288300 CCAMLR regions</td>
</tr>
<tr class="even">
<td>rgn_id</td>
<td>numeric</td>
<td>region ids; similar to rgn_ant_id, but Antartica/CCAMLR regions lumped as region 213</td>
<td>1-250 country land and eez (these are the official global regions; some numbers are skipped); 255 disputed land and eez; 260-277 fao high seas; 301-337 (country land, no eez)</td>
</tr>
<tr class="odd">
<td>rgn_name</td>
<td>character</td>
<td>country/territory name</td>
<td>e.g., Afghanistan, Belize, Prince Edward Islands</td>
</tr>
<tr class="even">
<td>rgn_key</td>
<td>factor</td>
<td>3 letter identification code</td>
<td>e.g., AFG, BEL</td>
</tr>
<tr class="odd">
<td>area_km2</td>
<td>numeric</td>
<td>area of region, km2</td>
<td>range of 1-30604795</td>
</tr>
</tbody>
</table>
<p><em>ohi_rasters</em></p>
<p>The ohi_rasters function returns two rasters, “zones” and “ocean”, both with ~1 km resolution and the mollweide coordinate reference system.</p>
<p>The “zones” raster cell values are the OHI region ID. The raster cell values can be linked to the region names using the region_data() function, and the rgn_ant_id variable from rgns_all.csv. This raster is typically used to extract pressure data for the eez regions.</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vQ7vOBar-Xr6QH8X36WsJidS0M_L4hhTQh6wgT4RTvIVJAgR04fWEqOP3oAbUcdg4JgWOu8sgzHhw0E/pub?w=960&amp;h=720" /></p>
<p>The “ocean” raster identifies ocean cells with a value of 1, and other cells are NA [NOTE: There is something weird about this raster in that it lists the values as 0, 255 (min, max), when in fact there are only 1 and NA values! If you need to convince yourself of this, you can use the <code>freq(ocean)</code> function to identify all cell values.]. This raster file is typically used to mask the ocean regions for pressure data.</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vTyex8lM_fxw6-23Z1mY59Ejf3PkcBUYOYtgVx7aqjNAqtPwEmIgG289EcY9B3wLlgvrp9tTA2iCRbO/pub?w=960&amp;h=720" /></p>
<p><em>region_data()</em></p>
<p>The region_data function returns two dataframes, “rgns_all” and “rgns_eez”.</p>
<p>The “rgns_all” dataframe includes data for the eez, fao, and ccamlr ocean regions. The IDs in rgn_ant_id correspond to the IDs in the zones raster. Once raster data are extracted for each region, the output is often aligned with the data in this dataframe.</p>
<p><em>Metadata for rgns_all dataframe</em></p>
<table>
<colgroup>
<col width="13%" />
<col width="23%" />
<col width="22%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>field</th>
<th>data type</th>
<th>description</th>
<th>examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>rgn_type</td>
<td>factor</td>
<td>similar to type_w_ant, but does not specify eez/ccamlr and types</td>
<td>eez (n=239), fao (15)</td>
</tr>
<tr class="even">
<td>type_w_ant</td>
<td>factor</td>
<td>identifies all ocean polygons as eez, fao (high seas), ccamlr (antarctica)</td>
<td>eez (n=220), fao (15), eez-ccamlr (19)</td>
</tr>
<tr class="odd">
<td>rgn_id</td>
<td>numeric</td>
<td>region ids; similar to rgn_ant_id, but Antartica/CCAMLR regions lumped as region 213</td>
<td>1-250 country eez (these are the official global regions; some numbers are skipped); 255 disputed eez; 260-277 fao high seas</td>
</tr>
<tr class="even">
<td>rgn_ant_id</td>
<td>numeric</td>
<td>region ids</td>
<td>1-250 country eez (these are the official global regions; some numbers are skipped); 255 disputed eez; 260-277 fao high seas; 248100-288300 CCAMLR regions</td>
</tr>
<tr class="odd">
<td>rgn_name</td>
<td>character</td>
<td>country/territory name</td>
<td>e.g., Afghanistan, Belize, Prince Edward Islands</td>
</tr>
</tbody>
</table>
<p>The “rgns_eez” dataframe includes data for the 220 OHI regions plus Antarctica (rgn_id 213). This file is used to make sure that all regions are included in dataprep files. It also includes data to indicate whether regions are territories. This can also be used for gapfilling (in some cases, it makes sense to assign territories the same value as their administrative country).</p>
<p><em>Metadata for rgns_eez dataframe</em></p>
<table>
<colgroup>
<col width="13%" />
<col width="22%" />
<col width="22%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>field</th>
<th>data type</th>
<th>description</th>
<th>examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>rgn_id</td>
<td>numeric</td>
<td>official global regions (except Antarctica, 213)</td>
<td>1-250</td>
</tr>
<tr class="even">
<td>rgn_name</td>
<td>character</td>
<td>country/territory name</td>
<td>e.g., Afghanistan, Belize, Prince Edward Islands</td>
</tr>
<tr class="odd">
<td>eez_iso3</td>
<td>factor</td>
<td>3 letter identification code</td>
<td>e.g., AFG, BEL</td>
</tr>
<tr class="even">
<td>territory</td>
<td>boolean</td>
<td>identifies whether the region is a territory</td>
<td>yes/no</td>
</tr>
<tr class="odd">
<td>admin_rgn_id</td>
<td>numeric</td>
<td>administrative country rgn_id if a territory, otherwise the rgn_id</td>
<td>1-250</td>
</tr>
<tr class="even">
<td>admin_country_name</td>
<td>character</td>
<td>administrative country name if a territory, otherwise the country name</td>
<td>e.g., Afghanistan, Belize, Canada</td>
</tr>
</tbody>
</table>
<p><em>region_syns()</em></p>
<p>Observed synonyms for each region, such that each region may have multiple synonyms. These data are used to convert outside data to the OHI region name and ID. This list is updated nearly everytime we run an assessment!</p>
<p><em>Metadata for region_syns dataframe</em></p>
<table>
<colgroup>
<col width="13%" />
<col width="23%" />
<col width="22%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>field</th>
<th>data type</th>
<th>description</th>
<th>examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>rgn_id</td>
<td>numeric</td>
<td>region ids</td>
<td>1-250</td>
</tr>
<tr class="even">
<td>rgn_name</td>
<td>character</td>
<td>country/territory name</td>
<td>e.g., Federated State of Micronesia; Micronesia, FS; Micronesia (Federated States of)</td>
</tr>
<tr class="odd">
<td>rgn_key</td>
<td>factor</td>
<td>2-letter code for countries</td>
<td>e.g., FM</td>
</tr>
<tr class="even">
<td>eez_iso3</td>
<td>factor</td>
<td>3-letter code for countries</td>
<td>e.g., FSM</td>
</tr>
<tr class="odd">
<td>rgn_typ</td>
<td>factor</td>
<td>status of region</td>
<td>disputed, landlocked, largescale (global, world); ohi_region</td>
</tr>
</tbody>
</table>
<p><em>low_pop()</em></p>
<p>Includes data for 21 regions with 0 and low populations. These data are used to identify regions that should have NA values because the goal does not apply to regions with no/low populations (e.g., livelihoods and economies).</p>
<p><em>Metadata for low_pop dataframe</em></p>
<table>
<colgroup>
<col width="13%" />
<col width="22%" />
<col width="22%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>field</th>
<th>data type</th>
<th>description</th>
<th>examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>rgn_id</td>
<td>numeric</td>
<td>region ids</td>
<td>1-250</td>
</tr>
<tr class="even">
<td>rgn_nam</td>
<td>character</td>
<td>country/territory name</td>
<td>e.g., Macquarie Island, Wake Island</td>
</tr>
<tr class="odd">
<td>Southern_Island</td>
<td>boolean</td>
<td>indicates if region is a southern island</td>
<td>1/0</td>
</tr>
<tr class="even">
<td>Inhabited</td>
<td>boolean</td>
<td>indicates if region is uninhabited</td>
<td>boolean, 1/0</td>
</tr>
<tr class="odd">
<td>est_population</td>
<td>numeric</td>
<td>number of established people in region</td>
<td>0-3000</td>
</tr>
</tbody>
</table>
<p><em>UNgeorgn()</em></p>
<p>Each global regions UN georegion based on social and geopolitical considerations.</p>
<p><em>Metadata for UNgeorgn dataframe</em></p>
<table>
<colgroup>
<col width="13%" />
<col width="22%" />
<col width="22%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th>field</th>
<th>data type</th>
<th>description</th>
<th>examples</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>rgn_id</td>
<td>numeric</td>
<td>region ids</td>
<td>1-250</td>
</tr>
<tr class="even">
<td>r0_label</td>
<td>factor</td>
<td>most inclusive category</td>
<td>World</td>
</tr>
<tr class="odd">
<td>r1_label</td>
<td>factor</td>
<td>7 classes</td>
<td>Africa (N=46), Americas (3), Asia (39), Europe (43), Latin America and the Caribbean (48), Oceana (34) Southern Islands (7)</td>
</tr>
<tr class="even">
<td>r2_label</td>
<td>factor</td>
<td>22 classes</td>
<td>e.g., New Zealand, Melanesia</td>
</tr>
<tr class="odd">
<td>rgn_label</td>
<td>character</td>
<td>global region name</td>
<td>e.g., Cocos Islands, Christmas Island</td>
</tr>
<tr class="even">
<td>Inhabited</td>
<td>boolean</td>
<td>indicates if region is uninhabited</td>
<td>boolean, 1/0</td>
</tr>
<tr class="odd">
<td>est_population</td>
<td>numeric</td>
<td>number of established people in region</td>
<td>0-3000</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="prep-rmd-5.-data-prep" class="section level2">
<h2><span class="header-section-number">3.8</span> prep Rmd: 5. Data prep</h2>
<p>We follow several coding practices when preparing our data:</p>
<ul>
<li><p>Code chunks are broken into manageable pieces and are proceeded by a description of what is being accomplished</p></li>
<li><p>The code within a chunk is documented to make it easier to follow and to help prevent errors. For example, expected values and results are described to help prevent potential errors (e.g., check to see all values between 0-1, length should be 0, etc.).</p></li>
<li><p>Run-on dplyr chains are avoided because they are impossible to follow and prone to error!</p></li>
<li><p>Data is checked throughout the dataprep process (checking dimensions of data after joins, special attention to missing data, etc.)</p></li>
<li><p>Intermediate data files are saved if they computationally take a long time to create or have information that could be useful in other aspects of the analysis.</p></li>
<li><p>The here package is used to standardize file paths.</p></li>
</ul>
<p>The final output should be a dataframe that includes:</p>
<ul>
<li><em>rgn_id</em> All 220 regions should be included in the final file!</li>
<li><em>year</em> This should include all years of data necessary to calculate scores for all scenario years, including trend. For some data layers the data has never been updated, in these cases, there is still a year column but it may only contain a single year.</li>
<li><em>value</em> This column will contain the calculated value for the data, and the column name will vary across datasets. In general, naming conventions should be consistently used every year, but feel free to modify the column name if you feel it could be improved. Just be sure to make the corresponding change to the “name_data_fld” in this file: <a href="https://github.com/OHI-Science/ohi-global/blob/draft/eez_layers_meta_data/layers_eez_base.csv" class="uri">https://github.com/OHI-Science/ohi-global/blob/draft/eez_layers_meta_data/layers_eez_base.csv</a>.</li>
</ul>
<p>This file should be saved as a .csv file in an “output” folder. In general, the name of the file should be the same as the previous year. However, if you feel the file name can be improved, you will need to update the fn variable in this data file:</p>
<p><a href="https://github.com/OHI-Science/ohi-global/blob/draft/eez_layers_meta_data/layers_eez_base.csv" class="uri">https://github.com/OHI-Science/ohi-global/blob/draft/eez_layers_meta_data/layers_eez_base.csv</a></p>
</div>
<div id="prep-rmd-6.-gapfilling" class="section level2">
<h2><span class="header-section-number">3.9</span> prep Rmd: 6. Gapfilling</h2>
<p>We make every effort to gapfill missing data. The only legitimate reason for a region to have an NA value is if the goal is not relevant to the region. For example, the livelihoods and economies goal is not relevant for an uninhabited island.</p>
<p>We have a paper describing why and how we estimate missing data: <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0160377" class="uri">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0160377</a>. But the following is the short version.</p>
<p>The first step to gapfilling missing data is to identify datasets that are predictive of the variable you are attempting to estimate and is also more complete. Here are some of the general approaches we use to estimate missing global data:</p>
<ul>
<li>Some mising data can be estimated from the original source. For example, if there are some missing years within a dataset, we can often estimate these values using a linear model of available data. For spatial raster data, we often estimate missing values using nearby raster cells.<br />
</li>
<li>Secondary datasets can also be used to estimate missing values. For example, we gapfill some missing Social Progress Index values using World Governance Index data as a predictor. And, in many cases we use <a href="https://github.com/OHI-Science/ohiprep_v2019/blob/gh-pages/globalprep/spatial/v2017/output/georegion_labels.csv">UN geopolitical data</a> that classifies countries based on geopolitical and social variables to predict missing data.</li>
<li>And, we often use multiple approaches to estimate missing values within a dataset.</li>
</ul>
<p>Our goal is to estimate missing data using the simplest model that best predicts the data. Given this, it is necessary to determine whether datasets and model terms actually predict missing data and to compare the performance of multiple models. Ideally cross-validation methods are used to evaluate how well the models predict missing data, otherwise models will appear to perform much better than they actually do.</p>
<p><em>There is no point in having a model with many predictive variables and complex non-linear fits if it performs no better than using a simple mean of the existing data!</em></p>
<div id="training-linear-models-for-gapfilling" class="section level3">
<h3><span class="header-section-number">3.9.1</span> Training linear models for gapfilling</h3>
<p><strong>Basic gist:</strong>
Use alternative datasets to predict data points for a dataset you have limited data points for. In this instance, we had FMI data points for 40 regions included in the OHI, and needed to gapfill the remaining 180 regions. We checked for correlation between the FMI data and several other datasets, including AO need (rescaled GDP per capita per person purchasing power), GDP per capita, social progress index (SPI), world governance indicator (WGI), and UN georegions.</p>
<p>A detailed example can be found <a href="https://github.com/OHI-Science/ohiprep_v2019/blob/gh-pages/globalprep/res_fmi/v2019/fmi_model_compare.Rmd">here</a></p>
<p><strong>Steps:</strong></p>
<ol style="list-style-type: decimal">
<li>Collect data that you’ll use to predict values, and merge with existing data (only use the values that correspond to data you already have to create this df)</li>
<li>Look at correlation between alternative data sets cor.test() and pairs.panels()</li>
<li>Create initial model:</li>
</ol>
<pre class="sourceCode r"><code class="sourceCode r">mod1 &lt;-<span class="st"> </span><span class="kw">lm</span>(dep_variable <span class="op">~</span><span class="st"> </span>ind_variable, <span class="dt">data=</span>df_name)</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Look at <code>summary(mod1)</code> to check for R-squared and p-value</li>
<li>Create an array of predicted values:</li>
</ol>
<pre class="sourceCode r"><code class="sourceCode r">df_predicted &lt;-<span class="st"> </span>df_name <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">group_by</span>(rgn_id) <span class="op">%&gt;%</span>
<span class="st">  </span>dplyr<span class="op">::</span><span class="kw">do</span>({
    pred &lt;-<span class="st"> </span><span class="kw">predict</span>(mod1, <span class="dt">newdata =</span>.[<span class="kw">c</span>(<span class="st">&#39;ind_variable&#39;</span>)]) <span class="co"># creates a new column for predicted values</span>
    <span class="kw">data.frame</span>(., fmi_pred) <span class="co"># do loop applies the model fitting and prediction to each country group</span>
    }) <span class="op">%&gt;%</span>
dplyr<span class="op">::</span><span class="kw">ungroup</span>()</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>Plot predicted vs actual values:</li>
</ol>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplotly</span>(<span class="kw">ggplot</span>(df_predicted, <span class="kw">aes</span>(<span class="dt">x =</span> dep_variable, <span class="dt">y =</span> pred, <span class="dt">labels =</span> rgn_id)) <span class="op">+</span>
<span class="kw">geom_point</span>() <span class="op">+</span>
<span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>))</code></pre>
<p><em>Note: “rgn_id” is just an example of a column in the dataset with a unique value for each observation.</em></p>
<p>lm() can also include multiple independent variables (must be merged into one data frame with one row for each observation):
mod2 &lt;- lm(dep_variable ~ ind_variable + ind_variable2, data=df_name)</p>
<ol start="7" style="list-style-type: decimal">
<li><p>Compare models for fit and degrees of freedom: <code>AIC(mod1, mod2)</code></p></li>
<li><p>Use best-fit model to gapfill areas without data using do loop as shown in step 4.</p></li>
</ol>
<p>If using UN georegions to gapfill, you’ll want to use the most specific/granular label possible. Use separate models for regional and continental levels (r2_label vs r1_label as described in metadata above), and write code to tell R to grab the data from the most granular data available. Example from <a href="https://github.com/OHI-Science/ohiprep_v2019/blob/gh-pages/globalprep/res_fmi/v2019/fmi_model_compare.Rmd">FMI model comparison .Rmd</a>:</p>
<pre class="sourceCode r"><code class="sourceCode r">fmi_gf_all &lt;-<span class="st"> </span>fmi_gf1 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">left_join</span>(fmi_gf2, <span class="dt">by=</span><span class="kw">c</span>(<span class="st">&quot;rgn_id&quot;</span>, <span class="st">&quot;r1_label&quot;</span>, <span class="st">&quot;r2_label&quot;</span>, <span class="st">&quot;rgn_label&quot;</span>, <span class="st">&quot;year&quot;</span>, <span class="st">&quot;spi&quot;</span>, <span class="st">&quot;fmi&quot;</span>)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">gapfilled =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(fmi) <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(fmi_pred_r2), <span class="st">&quot;1&quot;</span>, <span class="dv">0</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(fmi) <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(fmi_pred_r2), <span class="st">&quot;SPI + UN_geopolitical region r2&quot;</span>, <span class="ot">NA</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">fmi =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(fmi), fmi_pred_r2, fmi)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">gapfilled =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(fmi) <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(fmi_pred_r1), <span class="st">&quot;1&quot;</span>, gapfilled)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">method =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(fmi) <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(fmi_pred_r1), <span class="st">&quot;SPI + UN_geopolitical region r1&quot;</span>, method)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">fmi =</span> <span class="kw">ifelse</span>(<span class="kw">is.na</span>(fmi), fmi_pred_r1, fmi))</code></pre>
</div>
<div id="saving-gapfilling-report-files" class="section level3">
<h3><span class="header-section-number">3.9.2</span> Saving gapfilling report files</h3>
<p>Gapfilling steps are often mixed with the dataprep code. Once the gapfilling is done, we will save a dataset describing which data were gapfilled and the method. Every dataset should have a corresponding gapfilling dataset…<em>even if there was no gapfilling</em>.</p>
<p>The gapfilling record is saved with the same name as the corresponding output data, but with a "_gf" extension.</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vTGj33cUjqLbPw3sPkPBNGovaaradQASn3CzuTvLL8VgcFqcmlPGNskPksNjlqu1E8enu9zVSOklXJ8/pub?w=960&amp;h=720" /></p>
<p>The gapfilling dataset should mirror the corresponding dataset in regard to the number of rows and identifying columns (e.g., rgn_id, commodity, year, etc.). The gapfilling datset should include three additional columns that indicate the following:</p>
<ul>
<li>gapfilled: a zero or one value (or, NA if the value in the original data is NA) indicating whether the data was gapfilled or not.</li>
<li>method: a short descriptor of the method that was used (this should be described more fully somewhere in the documentation).</li>
<li>error: a numeric value describing the error around the estimate. This should be left blank if this was not determined (most of the time this will not be estimated).</li>
</ul>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vSFPuVZaRrO__JnOOSOl-AukPo3HN3rpBcIYDuC0pau2xHw3IVtnqOvTrdY5AfcVs7XybADvZD-tLtr/pub?w=691&amp;h=690" /></p>
</div>
</div>
<div id="prep-rmd-7.-results-check" class="section level2">
<h2><span class="header-section-number">3.10</span> prep Rmd: 7. Results check</h2>
<p>It is critical to check the final output data!</p>
<p>Here are some of the ways we check data:</p>
<ul>
<li>Is each region included in the final data?</li>
<li>Does the number of values for each region/year make sense (in most cases there should only be one value for each region/year)</li>
<li>Does the ranking of the values make sense? Do the high and low performing regions seem reasonable?</li>
<li>Do the number of NA values seem reasonable (ideally there will be none, unless the goal does not apply to that region)</li>
<li>Does the range/distribution of values seem reasonable, for example, some outputs should range from 0-1.</li>
</ul>
<p>The functions I use most commonly to check data are:
<code>hist()</code>, <code>summary()</code>, <code>dim()</code>, <code>table()</code></p>
<p>Because we calculate the global index every year, the most powerful way of checking our data is to compare the current data with last year’s data. In most cases, source data will remain the same from year to year (although some agencies will update previous data). Consequently, any changes will often reflect changes to the model and/or mistakes.</p>
<p>For this comparison we typically compare the most recent shared year of data between the two assessments. The following code walks through this process using data calculated for the lasting special places goal. In this case, we will pretend we have just completed preparing the data for the 2018 assessment and we want to compare the results to the 2017 assessment. Here we focus on the data for the 3nm offshore region.</p>
<pre><code>
library(dplyr)
library(here)
library(ggplot2)
library(plotly)

# want the region data from here:
source(&#39;http://ohi-science.org/ohiprep_v2019/workflow/R/common.R&#39;)

new &lt;- read.csv(here(&quot;globalprep/lsp/v2018/output/lsp_prot_area_offshore3nm.csv&quot;))
summary(new)

old &lt;- read.csv(here(&quot;globalprep/lsp/v2017/output/lsp_prot_area_offshore3nm.csv&quot;))
summary(old)
</code></pre>
<p>The most recent shared year between the two datasets is 2017, so this is the year that will be compared. The old and new values are plotted as a scatterplot, with a point for each region. We create an interactive plot and add a one-to-one line to make it easier to determine which regions have changed. Any changes in values will reflect either changes to the source data or changes to the data prep script.</p>
<p><em>It is important to explore the reasons for these changes because it could reflect errors!</em></p>
<pre><code>
# get region names
region_data()
head(rgns_eez)

compare &lt;- new %&gt;%
  rename(a_prot_3nm_new = a_prot_3nm) %&gt;%
  left_join(old, by=c(&quot;rgn_id&quot;, &quot;year&quot;)) %&gt;%
  filter(year == 2017) %&gt;%
  left_join(rgns_eez, by=&quot;rgn_id&quot;) %&gt;%
  select(rgn_id, rgn_name, year, a_prot_3nm, a_prot_3nm_new) %&gt;%
  mutate(region=paste(rgn_id, rgn_name, sep = &quot;-&quot;))

summary(compare)

compare_plot &lt;- ggplot(compare, aes(x=a_prot_3nm, y=a_prot_3nm_new, label=region)) +
  geom_point() +
  geom_abline(slope=1, intercept=0, color=&quot;red&quot;)
  
ggplotly(compare_plot)
</code></pre>
<p>In this case, the values for most regions remain the same from the 2017 to 2018 assessment, however, there are some changes. The biggest change was for Antarctica, which we can ignore because we do not calculate scores for Antarctica for the global assessment. However, there is also a fairly large change in the Greece data, and some smaller changes for some other regions.</p>
<p>By carefully exploring the source data we found that these differences reflect updates to the source data. For example, a marine reserve may have been established in 2017, but weren’t yet incorporated into the World Database on Protected Areas.</p>
</div>
<div id="prep-rmd-8.-final-run" class="section level2">
<h2><span class="header-section-number">3.11</span> prep Rmd: 8. Final run</h2>
<p>The final step is to commit and push all files. Then, close R and do NOT save the workspace image. The purpose of closing everything is to clear the memory of all data objects to make sure that the code still runs. This may be a problem if we change object names but fail to make necessary corresponding throughout the code.</p>
<p><img src="https://docs.google.com/drawings/d/e/2PACX-1vRf6bzIscOnKcqrxUNn9q4IOkPbElNbCJbcXpb3oC6jADbeVQ-e4pAA17fUBYa3GiJENILw48xgBSA4/pub?w=531&amp;h=243" /></p>
<p>I typically close everything by opening another repository.</p>
<p>Restart R and return to the ohiprep repo and rerun everything (except any really long processes).</p>
<p>Check the following:</p>
<ul>
<li>Does all the code still run?</li>
<li>Do you get the same results (check to see if output csv files are loaded to the Git window, this indicates that the data probably changed)?</li>
<li>Review warnings</li>
<li>Identify parts of the code you are unsure of, and have someone review</li>
</ul>
<hr />
</div>
<div id="notes-on-parallel-processing" class="section level2">
<h2><span class="header-section-number">3.12</span> Notes on parallel processing</h2>
<p>Coming soon!</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="starting-a-new-year.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="updating-scores.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/OHI-Science/ohi-global-guide/edit/master/data_prep.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["series.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
